#
# For licensing see accompanying LICENSE file.
# Copyright (C) 2024 Apple Inc. All Rights Reserved.
#

import os
import glob
import shutil
import torch
import itertools
import torchvision
import numpy as np
from PIL import Image
from tqdm import tqdm
from lpips import LPIPS
from loguru import logger
from scipy import linalg

from hugs.datasets.utils import (
    get_rotating_camera,
    get_smpl_canon_params,
    get_smpl_static_params, 
    get_static_camera
)
from hugs.losses.utils import ssim
from hugs.datasets import NeumanDataset
from hugs.losses.loss import HumanSceneLoss
from hugs.models.hugs_trimlp import HUGS_TRIMLP
from hugs.models.hugs_wo_trimlp import HUGS_WO_TRIMLP
from hugs.models import SceneGS
from hugs.utils.init_opt import optimize_init
from hugs.renderer.gs_renderer import render_human_scene
from hugs.utils.vis import save_ply
from hugs.utils.image import psnr, save_image
from hugs.utils.general import RandomIndexIterator, load_human_ckpt, save_images, create_video


class FIDCalculator:
    """Calculate Fréchet Inception Distance (FID) between real and generated images."""
    
    def __init__(self, device='cuda'):
        from torchvision.models import inception_v3, Inception_V3_Weights
        
        # Load pretrained InceptionV3
        self.model = inception_v3(weights=Inception_V3_Weights.DEFAULT, transform_input=False)
        self.model.fc = torch.nn.Identity()  # Remove classification head
        self.model = self.model.to(device).eval()
        self.device = device
        
        # Disable gradients
        for param in self.model.parameters():
            param.requires_grad = False
    
    @torch.no_grad()
    def extract_features(self, images):
        """
        Extract Inception features from images.
        Args:
            images: Tensor of shape [N, 3, H, W] in range [0, 1]
        Returns:
            features: Tensor of shape [N, 2048]
        """
        # Resize to 299x299 (InceptionV3 input size)
        if images.shape[-2:] != (299, 299):
            images = torch.nn.functional.interpolate(
                images, size=(299, 299), mode='bilinear', align_corners=False
            )
        
        # Normalize to [-1, 1] (ImageNet preprocessing)
        images = images * 2 - 1
        
        # Extract features
        features = self.model(images)
        return features
    
    def calculate_fid(self, real_images, fake_images):
        """
        Calculate FID between real and generated images.
        Args:
            real_images: List of tensors [3, H, W] in range [0, 1]
            fake_images: List of tensors [3, H, W] in range [0, 1]
        Returns:
            fid_score: float
        """
        # Extract features
        real_feats = []
        fake_feats = []
        
        # Process in batches to avoid OOM
        batch_size = 8
        for i in range(0, len(real_images), batch_size):
            batch_real = torch.stack(real_images[i:i+batch_size]).to(self.device)
            batch_fake = torch.stack(fake_images[i:i+batch_size]).to(self.device)
            
            real_feats.append(self.extract_features(batch_real).cpu())
            fake_feats.append(self.extract_features(batch_fake).cpu())
        
        real_feats = torch.cat(real_feats, dim=0).numpy()
        fake_feats = torch.cat(fake_feats, dim=0).numpy()
        
        # Calculate statistics
        mu_real = np.mean(real_feats, axis=0)
        sigma_real = np.cov(real_feats, rowvar=False)
        
        mu_fake = np.mean(fake_feats, axis=0)
        sigma_fake = np.cov(fake_feats, rowvar=False)
        
        # Calculate FID
        fid = self._calculate_frechet_distance(mu_real, sigma_real, mu_fake, sigma_fake)
        return fid
    
    @staticmethod
    def _calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):
        """Calculate Fréchet distance between two Gaussians."""
        mu1 = np.atleast_1d(mu1)
        mu2 = np.atleast_1d(mu2)
        
        sigma1 = np.atleast_2d(sigma1)
        sigma2 = np.atleast_2d(sigma2)
        
        diff = mu1 - mu2
        
        # Product might be almost singular
        covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)
        if not np.isfinite(covmean).all():
            logger.warning(f"FID calculation produced singular product; adding {eps} to diagonal of cov estimates")
            offset = np.eye(sigma1.shape[0]) * eps
            covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))
        
        # Numerical error might give slight imaginary component
        if np.iscomplexobj(covmean):
            if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):
                m = np.max(np.abs(covmean.imag))
                raise ValueError(f"Imaginary component {m}")
            covmean = covmean.real
        
        tr_covmean = np.trace(covmean)
        
        return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean


def get_train_dataset(cfg):
    if cfg.dataset.name == 'neuman':
        logger.info(f'Loading NeuMan dataset {cfg.dataset.seq}-train')
        dataset = NeumanDataset(
            cfg.dataset.seq, 'train', 
            render_mode=cfg.mode,
            add_bg_points=cfg.scene.add_bg_points,
            num_bg_points=cfg.scene.num_bg_points,
            bg_sphere_dist=cfg.scene.bg_sphere_dist,
            clean_pcd=cfg.scene.clean_pcd,
            cloth_upper=getattr(cfg.dataset, 'cloth_upper', 'tshirt'),
            cloth_lower=getattr(cfg.dataset, 'cloth_lower', 'pants'),
        )
    elif cfg.dataset.name == 'zju':
        from hugs.datasets.zjumocap import ZJUMoCapDataset
        logger.info(f'Loading ZJU-MoCap dataset {cfg.dataset.subject}-train (camera {cfg.dataset.camera_id})')
        dataset = ZJUMoCapDataset(
            cfg.dataset.subject, 'train',
            render_mode=cfg.mode,
            camera_id=getattr(cfg.dataset, 'camera_id', 0),
            add_bg_points=cfg.scene.add_bg_points,
            num_bg_points=cfg.scene.num_bg_points,
            bg_sphere_dist=cfg.scene.bg_sphere_dist,
            clean_pcd=cfg.scene.clean_pcd,
            cloth_upper=getattr(cfg.dataset, 'cloth_upper', 'tshirt'),
            cloth_lower=getattr(cfg.dataset, 'cloth_lower', 'pants'),
        )
    
    return dataset


def get_val_dataset(cfg):
    if cfg.dataset.name == 'neuman':
        logger.info(f'Loading NeuMan dataset {cfg.dataset.seq}-val')
        dataset = NeumanDataset(
            cfg.dataset.seq, 'val', cfg.mode,
            cloth_upper=getattr(cfg.dataset, 'cloth_upper', 'tshirt'),
            cloth_lower=getattr(cfg.dataset, 'cloth_lower', 'pants'),
        )
    elif cfg.dataset.name == 'zju':
        from hugs.datasets.zjumocap import ZJUMoCapDataset
        logger.info(f'Loading ZJU-MoCap dataset {cfg.dataset.subject}-val (camera {cfg.dataset.camera_id})')
        dataset = ZJUMoCapDataset(
            cfg.dataset.subject, 'val', cfg.mode,
            camera_id=getattr(cfg.dataset, 'camera_id', 0),
            cloth_upper=getattr(cfg.dataset, 'cloth_upper', 'tshirt'),
            cloth_lower=getattr(cfg.dataset, 'cloth_lower', 'pants'),
        )
   
    return dataset


def get_anim_dataset(cfg):
    if cfg.dataset.name == 'neuman':
        logger.info(f'Loading NeuMan dataset {cfg.dataset.seq}-anim')
        dataset = NeumanDataset(
            cfg.dataset.seq, 'anim', cfg.mode,
            cloth_upper=getattr(cfg.dataset, 'cloth_upper', 'tshirt'),
            cloth_lower=getattr(cfg.dataset, 'cloth_lower', 'pants'),
        )
    elif cfg.dataset.name == 'zju':
        dataset = None
        
    return dataset


class GaussianTrainer():
    def __init__(self, cfg) -> None:
        self.cfg = cfg
        
        # get dataset
        if not cfg.eval:
            self.train_dataset = get_train_dataset(cfg)
        self.val_dataset = get_val_dataset(cfg)
        self.anim_dataset = get_anim_dataset(cfg)
        
        self.eval_metrics = {}
        self.lpips = LPIPS(net="alex", pretrained=True).to('cuda')
        self.fid_calculator = FIDCalculator(device='cuda')
        # get models
        self.human_gs, self.scene_gs = None, None
        
        if cfg.mode in ['human', 'human_scene']:
            if cfg.human.name == 'hugs_wo_trimlp':
                self.human_gs = HUGS_WO_TRIMLP(
                    sh_degree=cfg.human.sh_degree, 
                    n_subdivision=cfg.human.n_subdivision,  
                    use_surface=cfg.human.use_surface,
                    init_2d=cfg.human.init_2d,
                    rotate_sh=cfg.human.rotate_sh,
                    isotropic=cfg.human.isotropic,
                    init_scale_multiplier=cfg.human.init_scale_multiplier,
                )
                init_betas = torch.stack([x['betas'] for x in self.train_dataset.cached_data], dim=0)
                self.human_gs.create_betas(init_betas[0], cfg.human.optim_betas)
                self.human_gs.initialize()
            elif cfg.human.name == 'hugs_trimlp':
                init_betas = torch.stack([x['betas'] for x in self.val_dataset.cached_data], dim=0)
                self.human_gs = HUGS_TRIMLP(
                    sh_degree=cfg.human.sh_degree, 
                    n_subdivision=cfg.human.n_subdivision,  
                    use_surface=cfg.human.use_surface,
                    init_2d=cfg.human.init_2d,
                    rotate_sh=cfg.human.rotate_sh,
                    isotropic=cfg.human.isotropic,
                    init_scale_multiplier=cfg.human.init_scale_multiplier,
                    n_features=32,
                    use_deformer=cfg.human.use_deformer,
                    disable_posedirs=cfg.human.disable_posedirs,
                    triplane_res=cfg.human.triplane_res,
                    betas=init_betas[0]
                )
                self.human_gs.create_betas(init_betas[0], cfg.human.optim_betas)
                if not cfg.eval:
                    self.human_gs.initialize()
                    self.human_gs = optimize_init(self.human_gs, num_steps=5000)
        # Initialize cloth if dataset provides cloth mesh
        if hasattr(self.train_dataset, "cloth_vertices") and hasattr(self.train_dataset, "cloth_faces"):
            cloth_vertices = self.train_dataset.cloth_vertices
            cloth_faces = self.train_dataset.cloth_faces
            self.human_gs.initialize_cloth(cloth_vertices, cloth_faces)

        if cfg.mode in ['scene', 'human_scene']:
            self.scene_gs = SceneGS(
                sh_degree=cfg.scene.sh_degree,
            )
            
        # setup the optimizers
        if self.human_gs:
            self.human_gs.setup_optimizer(cfg=cfg.human.lr)
            logger.info(self.human_gs)
            if cfg.human.ckpt:
                # load_human_ckpt(self.human_gs, cfg.human.ckpt)
                self.human_gs.load_state_dict(torch.load(cfg.human.ckpt))
                logger.info(f'Loaded human model from {cfg.human.ckpt}')
            else:
                ckpt_files = sorted(glob.glob(f'{cfg.logdir_ckpt}/*human*.pth'))
                if len(ckpt_files) > 0:
                    ckpt = torch.load(ckpt_files[-1])
                    self.human_gs.load_state_dict(ckpt)
                    logger.info(f'Loaded human model from {ckpt_files[-1]}')

            if not cfg.eval:
                init_smpl_global_orient = torch.stack([x['global_orient'] for x in self.train_dataset.cached_data])
                init_smpl_body_pose = torch.stack([x['body_pose'] for x in self.train_dataset.cached_data])
                init_smpl_trans = torch.stack([x['transl'] for x in self.train_dataset.cached_data], dim=0)
                init_betas = torch.stack([x['betas'] for x in self.train_dataset.cached_data], dim=0)
                init_eps_offsets = torch.zeros((len(self.train_dataset), self.human_gs.n_gs, 3), 
                                            dtype=torch.float32, device="cuda")

                self.human_gs.create_betas(init_betas[0], cfg.human.optim_betas)
                
                self.human_gs.create_body_pose(init_smpl_body_pose, cfg.human.optim_pose)
                self.human_gs.create_global_orient(init_smpl_global_orient, cfg.human.optim_pose)
                self.human_gs.create_transl(init_smpl_trans, cfg.human.optim_trans)
                
                self.human_gs.setup_optimizer(cfg=cfg.human.lr)
                    
        if self.scene_gs:
            logger.info(self.scene_gs)
            if cfg.scene.ckpt:
                ckpt = torch.load(cfg.scene.ckpt)
                self.scene_gs.restore(ckpt, cfg.scene.lr)
                logger.info(f'Loaded scene model from {cfg.scene.ckpt}')
            else:
                ckpt_files = sorted(glob.glob(f'{cfg.logdir_ckpt}/*scene*.pth'))
                if len(ckpt_files) > 0:
                    ckpt = torch.load(ckpt_files[-1])
                    self.scene_gs.restore(ckpt, cfg.scene.lr)
                    logger.info(f'Loaded scene model from {cfg.scene.ckpt}')
                else:
                    pcd = self.train_dataset.init_pcd
                    spatial_lr_scale = self.train_dataset.radius
                    self.scene_gs.create_from_pcd(pcd, spatial_lr_scale)
                
            self.scene_gs.setup_optimizer(cfg=cfg.scene.lr)
        
        bg_color = cfg.bg_color
        if bg_color == 'white':
            self.bg_color = torch.tensor([1, 1, 1], dtype=torch.float32, device="cuda")
        elif bg_color == 'black':
            self.bg_color = torch.tensor([0, 0, 0], dtype=torch.float32, device="cuda")
        else:
            raise ValueError(f"Unknown background color {bg_color}")
        
        if cfg.mode in ['human', 'human_scene']:
            l = cfg.human.loss

            self.loss_fn = HumanSceneLoss(
                l_ssim_w=l.ssim_w,
                l_l1_w=l.l1_w,
                l_lpips_w=l.lpips_w,
                l_lbs_w=l.lbs_w,
                l_humansep_w=l.humansep_w,
                l_cloth_sim_w=getattr(l, "cloth_sim_w", 0.0),
                l_cloth_arap_w=getattr(l, "cloth_arap_w", 0.0),
                l_cloth_mask_w=getattr(l, "cloth_mask_w", 0.0),
                l_opacity_entropy_w=getattr(l, "opacity_entropy_w", 0.0),
                l_tv_w=getattr(l, "tv_w", 0.0),
                num_patches=l.num_patches,
                patch_size=l.patch_size,
                use_patches=l.use_patches,
                bg_color=self.bg_color,
            )
        else:
            self.cfg.train.optim_scene = True
            l = cfg.scene.loss
            self.loss_fn = HumanSceneLoss(
                l_ssim_w=l.ssim_w,
                l_l1_w=l.l1_w,
                bg_color=self.bg_color,
            )
                
        if cfg.mode in ['human', 'human_scene']:
            self.canon_camera_params = get_rotating_camera(
                dist=5.0, img_size=512, 
                nframes=cfg.human.canon_nframes, device='cuda',
                angle_limit=2*torch.pi,
            )
            betas = self.human_gs.betas.detach() if hasattr(self.human_gs, 'betas') else self.train_dataset.betas[0]
            self.static_smpl_params = get_smpl_static_params(
                betas=betas,
                pose_type=self.cfg.human.canon_pose_type
            )

    def train(self):
        if self.human_gs:
            self.human_gs.train()

        pbar = tqdm(range(self.cfg.train.num_steps+1), desc="Training")
        
        rand_idx_iter = RandomIndexIterator(len(self.train_dataset))
        sgrad_means, sgrad_stds = [], []
        for t_iter in range(self.cfg.train.num_steps+1):
            render_mode = self.cfg.mode
            
            if self.scene_gs and self.cfg.train.optim_scene:
                self.scene_gs.update_learning_rate(t_iter)
            
            if hasattr(self.human_gs, 'update_learning_rate'):
                self.human_gs.update_learning_rate(t_iter)
            for param_group in self.human_gs.optimizer.param_groups:
                if param_group.get("name", "") == "cloth_xyz":
                    lr = self.human_gs.xyz_scheduler_args(t_iter)
                    param_group["lr"] = lr

            rnd_idx = next(rand_idx_iter)
            data = self.train_dataset[rnd_idx]
            
            human_pack, scene_gs_out = None, None
            
            if self.human_gs:
                human_pack = self.human_gs.forward(
                    smpl_scale=data['smpl_scale'][None],
                    dataset_idx=rnd_idx,
                    is_train=True,
                    ext_tfs=None,
                )

                body_out  = human_pack["body"] if isinstance(human_pack, dict) else human_pack
                cloth_out = human_pack.get("cloth", None) if isinstance(human_pack, dict) else None
            else:
                body_out, cloth_out = None, None

            
            if self.scene_gs:
                if t_iter >= self.cfg.scene.opt_start_iter:
                    scene_gs_out = self.scene_gs.forward()
                else:
                    render_mode = 'human'
            else:
                scene_gs_out = None
                render_mode = 'human'
            
            bg_color = torch.rand(3, dtype=torch.float32, device="cuda")
            
            
            if self.cfg.human.loss.humansep_w > 0.0 and render_mode == 'human_scene':
                render_human_separate = True
                human_bg_color = torch.rand(3, dtype=torch.float32, device="cuda")
            else:
                human_bg_color = None
                render_human_separate = False
            
            render_pkg = render_human_scene(
                data=data,
                human_gs_out=body_out,
                scene_gs_out=scene_gs_out,
                bg_color=bg_color,
                human_bg_color=human_bg_color,
                render_mode=render_mode,
                render_human_separate=render_human_separate,
                cloth_gs_out=cloth_out,
            )

            
            if self.human_gs:
                self.human_gs.init_values['edges'] = self.human_gs.edges
                if hasattr(self.human_gs, "cloth_gaussians") and "edges" in self.human_gs.cloth_gaussians:
                    self.human_gs.init_values['cloth_edges'] = self.human_gs.cloth_gaussians["edges"]
        
            loss, loss_dict, loss_extras = self.loss_fn(
                data,
                render_pkg,
                body_out,
                cloth_gs_out=cloth_out, 
                render_mode=render_mode,
                human_gs_init_values=self.human_gs.init_values if self.human_gs else None,
                bg_color=bg_color,
                human_bg_color=human_bg_color,
            )
            
            loss.backward()
            
            loss_dict['loss'] = loss
            
            if t_iter % 10 == 0:
                postfix_dict = {
                    "#hp": f"{self.human_gs.n_gs/1000 if self.human_gs else 0:.1f}K",
                    "#sp": f"{self.scene_gs.get_xyz.shape[0]/1000 if self.scene_gs else 0:.1f}K",
                    'h_sh_d': self.human_gs.active_sh_degree if self.human_gs else 0,
                    's_sh_d': self.scene_gs.active_sh_degree if self.scene_gs else 0,
                }
                for k, v in loss_dict.items():
                    postfix_dict["l_"+k] = f"{v.item():.4f}"
                        
                pbar.set_postfix(postfix_dict)
                pbar.update(10)
                
            if t_iter == self.cfg.train.num_steps:
                pbar.close()

            if t_iter % 1000 == 0:
                with torch.no_grad():
                    pred_img = loss_extras['pred_img']
                    gt_img = loss_extras['gt_img']
                    log_pred_img = (pred_img.cpu().numpy().transpose(1, 2, 0) * 255).astype(np.uint8)
                    log_gt_img = (gt_img.cpu().numpy().transpose(1, 2, 0) * 255).astype(np.uint8)
                    log_img = np.concatenate([log_gt_img, log_pred_img], axis=1)
                    save_images(log_img, f'{self.cfg.logdir}/train/{t_iter:06d}.png')
                    
                    # === Save Body and Cloth Meshes After LBS Deformation ===
                    logger.info(f"Debug - cloth_out is None: {cloth_out is None}")
                    logger.info(f"Debug - cloth_gt in data: {'cloth_gt' in data}")
                    
                    # Save body mesh after LBS deformation
                    if 'body' in human_pack and human_pack['body'] is not None:
                        body_out = human_pack['body']
                        if 'xyz' in body_out:
                            body_deformed_ply_path = f'{self.cfg.logdir}/meshes/body_deformed_{t_iter:06d}.ply'
                            save_ply(body_out, body_deformed_ply_path)
                            logger.info(f"Saved body after LBS deformation to {body_deformed_ply_path}")
                            
                            # Also save canonical body for comparison
                            if 'xyz_canon' in body_out:
                                body_canon_ply_path = f'{self.cfg.logdir}/meshes/body_canon_{t_iter:06d}.ply'
                                save_ply({'xyz': body_out['xyz_canon']}, body_canon_ply_path)
                                logger.info(f"Saved canonical body to {body_canon_ply_path}")
                                
                                # Check body deformation magnitude
                                body_deformation = torch.norm(body_out['xyz'] - body_out['xyz_canon'], dim=1).mean()
                                logger.info(f"Body deformation magnitude: {body_deformation:.6f}")
                    
                    # Save cloth mesh after LBS deformation
                    if cloth_out is not None:
                        logger.info(f"Debug - cloth_out keys: {list(cloth_out.keys())}")
                        if 'xyz' in cloth_out:
                            # Save predicted deformed cloth
                            cloth_pred_xyz = cloth_out["xyz"].detach().cpu().numpy()
                            cloth_pred_ply_path = f'{self.cfg.logdir}/meshes/cloth_deformed_{t_iter:06d}.ply'
                            save_ply(cloth_out, cloth_pred_ply_path)
                            logger.info(f"Saved cloth after LBS deformation to {cloth_pred_ply_path}")
                            
                            # Also save canonical cloth for comparison
                            if 'xyz_canon' in cloth_out:
                                cloth_canon_xyz = cloth_out["xyz_canon"].detach().cpu().numpy()
                                cloth_canon_ply_path = f'{self.cfg.logdir}/meshes/cloth_canon_{t_iter:06d}.ply'
                                save_ply({'xyz': torch.from_numpy(cloth_canon_xyz)}, cloth_canon_ply_path)
                                logger.info(f"Saved canonical cloth to {cloth_canon_ply_path}")
                                
                                # Check if deformation is actually happening
                                deformation_magnitude = np.linalg.norm(cloth_pred_xyz - cloth_canon_xyz, axis=1).mean()
                                logger.info(f"Cloth deformation magnitude: {deformation_magnitude:.6f}")
                                if deformation_magnitude < 1e-6:
                                    logger.warning("⚠️ Cloth deformation is negligible - LBS might not be working!")
                                
                                # === MESH ANALYSIS ===
                                logger.info("=== CLOTH MESH ANALYSIS ===")
                                logger.info(f"Predicted cloth: {cloth_pred_xyz.shape[0]} vertices, bounds: [{cloth_pred_xyz.min(axis=0)}, {cloth_pred_xyz.max(axis=0)}]")
                                logger.info(f"Canonical cloth: {cloth_canon_xyz.shape[0]} vertices, bounds: [{cloth_canon_xyz.min(axis=0)}, {cloth_canon_xyz.max(axis=0)}]")
                                
                                # Calculate mesh statistics
                                pred_center = cloth_pred_xyz.mean(axis=0)
                                canon_center = cloth_canon_xyz.mean(axis=0)
                                pred_scale = np.linalg.norm(cloth_pred_xyz - pred_center, axis=1).max()
                                canon_scale = np.linalg.norm(cloth_canon_xyz - canon_center, axis=1).max()
                                
                                logger.info(f"Pred center: {pred_center}, scale: {pred_scale:.4f}")
                                logger.info(f"Canon center: {canon_center}, scale: {canon_scale:.4f}")
                                logger.info(f"Center shift: {np.linalg.norm(pred_center - canon_center):.4f}")
                                logger.info(f"Scale ratio: {pred_scale/canon_scale:.4f}")
                        
                        # Save GT deformed cloth if available
                        if 'cloth_gt' in data:
                            cloth_gt_xyz = data['cloth_gt'].detach().cpu().numpy()
                            cloth_gt_ply_path = f'{self.cfg.logdir}/meshes/cloth_gt_ply_{t_iter:06d}.ply'
                            save_ply({'xyz': torch.from_numpy(cloth_gt_xyz)}, cloth_gt_ply_path)
                            logger.info(f"Saved GT cloth to {cloth_gt_ply_path}")
                            
                            # === GT MESH ANALYSIS ===
                            logger.info("=== GT CLOTH MESH ANALYSIS ===")
                            logger.info(f"GT cloth: {cloth_gt_xyz.shape[0]} vertices, bounds: [{cloth_gt_xyz.min(axis=0)}, {cloth_gt_xyz.max(axis=0)}]")
                            
                            # Calculate GT mesh statistics
                            gt_center = cloth_gt_xyz.mean(axis=0)
                            gt_scale = np.linalg.norm(cloth_gt_xyz - gt_center, axis=1).max()
                            logger.info(f"GT center: {gt_center}, scale: {gt_scale:.4f}")
                            
                            # Compare with predicted mesh
                            if 'xyz' in cloth_out:
                                pred_center = cloth_pred_xyz.mean(axis=0)
                                pred_scale = np.linalg.norm(cloth_pred_xyz - pred_center, axis=1).max()
                                
                                logger.info("=== PRED vs GT COMPARISON ===")
                                logger.info(f"Vertex count: Pred={cloth_pred_xyz.shape[0]}, GT={cloth_gt_xyz.shape[0]}")
                                logger.info(f"Center diff: {np.linalg.norm(pred_center - gt_center):.4f}")
                                logger.info(f"Scale ratio: {pred_scale/gt_scale:.4f}")
                                
                                # Check if meshes are in same coordinate frame
                                center_aligned_pred = cloth_pred_xyz - pred_center + gt_center
                                scale_aligned_pred = center_aligned_pred * (gt_scale / pred_scale)
                                
                                # Quick Chamfer distance (unnormalized)
                                pred_tensor = torch.from_numpy(scale_aligned_pred).float()
                                gt_tensor = torch.from_numpy(cloth_gt_xyz).float()
                                chamfer_raw = torch.cdist(pred_tensor, gt_tensor).min(dim=1)[0].mean().item()
                                logger.info(f"Raw Chamfer distance (after alignment): {chamfer_raw:.6f}")
                                
                                # Determine mesh relationship
                                if abs(pred_scale/gt_scale - 1.0) < 0.1 and np.linalg.norm(pred_center - gt_center) < 0.1:
                                    logger.info("✅ Meshes appear to be in same coordinate frame")
                                else:
                                    logger.info("⚠️ Meshes need coordinate alignment")
                                
                                if abs(cloth_pred_xyz.shape[0] / cloth_gt_xyz.shape[0] - 1.0) < 0.1:
                                    logger.info("✅ Similar vertex counts - can use vertex-wise loss")
                                else:
                                    logger.info("⚠️ Different vertex counts - need Chamfer/point-to-surface loss")
                        else:
                            logger.warning("cloth_gt not found in data")
                    else:
                        logger.warning("cloth_out is None - cloth not being generated")
            
            if t_iter >= self.cfg.scene.opt_start_iter:
                if (t_iter - self.cfg.scene.opt_start_iter) < self.cfg.scene.densify_until_iter and self.cfg.mode in ['scene', 'human_scene']:
                    render_pkg['scene_viewspace_points'] = render_pkg['viewspace_points']
                    render_pkg['scene_viewspace_points'].grad = render_pkg['viewspace_points'].grad
                        
                    sgrad_mean, sgrad_std = render_pkg['scene_viewspace_points'].grad.mean(), render_pkg['scene_viewspace_points'].grad.std()
                    sgrad_means.append(sgrad_mean.item())
                    sgrad_stds.append(sgrad_std.item())
                    with torch.no_grad():
                        self.scene_densification(
                            visibility_filter=render_pkg['scene_visibility_filter'],
                            radii=render_pkg['scene_radii'],
                            viewspace_point_tensor=render_pkg['scene_viewspace_points'],
                            iteration=(t_iter - self.cfg.scene.opt_start_iter) + 1,
                        )
                        
            if t_iter < self.cfg.human.densify_until_iter and self.cfg.mode in ['human', 'human_scene']:
                render_pkg['human_viewspace_points'] = render_pkg['viewspace_points'][:body_out['xyz'].shape[0]]
                render_pkg['human_viewspace_points'].grad = render_pkg['viewspace_points'].grad[:body_out['xyz'].shape[0]]
                with torch.no_grad():
                    self.human_densification(
                        human_gs_out=body_out,
                        visibility_filter=render_pkg['human_visibility_filter'],
                        radii=render_pkg['human_radii'],
                        viewspace_point_tensor=render_pkg['human_viewspace_points'],
                        iteration=t_iter+1,
                    )
            # === CLOTH DENSIFICATION (moved before optimizer step) ===
            if hasattr(self.human_gs, "cloth_gaussians") and self.human_gs.cloth_gaussians is not None and cloth_out is not None:
                # Extract cloth viewspace points (if rendered separately with gradients)
                if 'cloth_visibility_filter' in render_pkg and 'cloth_radii' in render_pkg:
                    # Accumulate cloth radii stats
                    self.human_gs.cloth_max_radii2D[render_pkg['cloth_visibility_filter']] = torch.max(
                        self.human_gs.cloth_max_radii2D[render_pkg['cloth_visibility_filter']],
                        render_pkg['cloth_radii'][render_pkg['cloth_visibility_filter']]
                    )
                    
                    # Accumulate cloth viewspace gradients
                    if 'cloth_viewspace_points' in render_pkg and render_pkg['cloth_viewspace_points'].grad is not None:
                        self.human_gs.add_cloth_densification_stats(
                            render_pkg['cloth_viewspace_points'],
                            render_pkg['cloth_visibility_filter']
                        )
                    
                # Densify periodically, not every iteration
                if t_iter > self.cfg.human.densify_from_iter and t_iter % self.cfg.human.densification_interval == 0:
                    try:
                        with torch.no_grad():  # Ensure no gradients during densification
                            self.human_gs.cloth_densify_and_prune(
                                cloth_gs_out=cloth_out,
                                grad_threshold=self.cfg.human.densify_grad_threshold,
                                min_opacity=self.cfg.human.prune_min_opacity,
                                extent=self.cfg.human.densify_extent,
                                max_screen_size=20,
                                max_n_gs=self.cfg.human.max_n_gaussians,
                            )
                            logger.info(f"[Iter {t_iter}] Cloth densification completed successfully")
                    except Exception as e:
                        logger.error(f"[Iter {t_iter}] Cloth densification failed: {e}")
                        # Continue training without densification for this iteration

            if self.human_gs:
                self.human_gs.optimizer.step()
                self.human_gs.optimizer.zero_grad(set_to_none=True)
                
            if self.scene_gs and self.cfg.train.optim_scene:
                if t_iter >= self.cfg.scene.opt_start_iter:
                    self.scene_gs.optimizer.step()
                    self.scene_gs.optimizer.zero_grad(set_to_none=True)
                
            # save checkpoint Save every 1000 iters from 14000 onwards, OR at final iteration
            if ((t_iter >= 14000 and t_iter % self.cfg.train.save_ckpt_interval == 0) or \
                (t_iter == self.cfg.train.num_steps and t_iter > 0)):
                self.save_ckpt(t_iter)

            # run validation
            if t_iter % self.cfg.train.val_interval == 0 and t_iter > 0:
                self.validate(t_iter)
            
            if t_iter == 0:
                if self.scene_gs:
                    self.scene_gs.save_ply(f'{self.cfg.logdir}/meshes/scene_{t_iter:06d}_splat.ply')
                if self.human_gs:
                    save_ply(body_out, f'{self.cfg.logdir}/meshes/human_{t_iter:06d}_splat.ply')
                    save_ply(cloth_out, f'{self.cfg.logdir}/meshes/cloth_{t_iter:06d}_splat.ply')
                if self.cfg.mode in ['human', 'human_scene']:
                    self.render_canonical(t_iter, nframes=self.cfg.human.canon_nframes)
                
            if t_iter % self.cfg.train.anim_interval == 0 and t_iter > 0 and self.cfg.train.anim_interval > 0:
                if self.human_gs:
                    save_ply(body_out, f'{self.cfg.logdir}/meshes/human_{t_iter:06d}_splat.ply')
                    save_ply(cloth_out, f'{self.cfg.logdir}/meshes/cloth_{t_iter:06d}_splat.ply')
                if self.anim_dataset is not None:
                    self.animate(t_iter)
                    
                if self.cfg.mode in ['human', 'human_scene']:
                    self.render_canonical(t_iter, nframes=self.cfg.human.canon_nframes)
            
            if t_iter % 1000 == 0 and t_iter > 0:
                if self.human_gs: self.human_gs.oneupSHdegree()
                if self.scene_gs: self.scene_gs.oneupSHdegree()
                
            if self.cfg.train.save_progress_images and t_iter % self.cfg.train.progress_save_interval == 0 and self.cfg.mode in ['human', 'human_scene']:
                self.render_canonical(t_iter, nframes=2, is_train_progress=True)
        
        # train progress images
        if self.cfg.train.save_progress_images:
            video_fname = f'{self.cfg.logdir}/train_{self.cfg.dataset.name}_{self.cfg.dataset.seq}.mp4'
            create_video(f'{self.cfg.logdir}/train_progress/', video_fname, fps=10)
            shutil.rmtree(f'{self.cfg.logdir}/train_progress/')
            
    def save_ckpt(self, iter=None):
        
        iter_s = 'final' if iter is None else f'{iter:06d}'
        
        if self.human_gs:
            torch.save(self.human_gs.state_dict(), f'{self.cfg.logdir_ckpt}/human_{iter_s}.pth')
            
        if self.scene_gs:
            torch.save(self.scene_gs.state_dict(), f'{self.cfg.logdir_ckpt}/scene_{iter_s}.pth')
            self.scene_gs.save_ply(f'{self.cfg.logdir}/meshes/scene_{iter_s}_splat.ply')
            
        logger.info(f'Saved checkpoint {iter_s}')
                
    def scene_densification(self, visibility_filter, radii, viewspace_point_tensor, iteration):
        self.scene_gs.max_radii2D[visibility_filter] = torch.max(
            self.scene_gs.max_radii2D[visibility_filter], 
            radii[visibility_filter]
        )
        self.scene_gs.add_densification_stats(viewspace_point_tensor, visibility_filter)

        if iteration > self.cfg.scene.densify_from_iter and iteration % self.cfg.scene.densification_interval == 0:
            size_threshold = 20 if iteration > self.cfg.scene.opacity_reset_interval else None
            self.scene_gs.densify_and_prune(
                self.cfg.scene.densify_grad_threshold, 
                min_opacity=self.cfg.scene.prune_min_opacity, 
                extent=self.train_dataset.radius, 
                max_screen_size=size_threshold,
                max_n_gs=self.cfg.scene.max_n_gaussians,
            )
        
        is_white = self.bg_color.sum().item() == 3.
        
        if iteration % self.cfg.scene.opacity_reset_interval == 0 or (is_white and iteration == self.cfg.scene.densify_from_iter):
            logger.info(f"[{iteration:06d}] Resetting opacity!!!")
            self.scene_gs.reset_opacity()
    
    def human_densification(self, human_gs_out, visibility_filter, radii, viewspace_point_tensor, iteration):
        self.human_gs.max_radii2D[visibility_filter] = torch.max(
            self.human_gs.max_radii2D[visibility_filter], 
            radii[visibility_filter]
        )
        
        self.human_gs.add_densification_stats(viewspace_point_tensor, visibility_filter)

        if iteration > self.cfg.human.densify_from_iter and iteration % self.cfg.human.densification_interval == 0:
            size_threshold = 20
            self.human_gs.densify_and_prune(
                human_gs_out,
                self.cfg.human.densify_grad_threshold, 
                min_opacity=self.cfg.human.prune_min_opacity, 
                extent=self.cfg.human.densify_extent, 
                max_screen_size=size_threshold,
                max_n_gs=self.cfg.human.max_n_gaussians,
            )
    
    @torch.no_grad()
    def validate(self, iter=None):
        
        iter_s = 'final' if iter is None else f'{iter:06d}'
        
        bg_color = torch.zeros(3, dtype=torch.float32, device="cuda")

        if self.human_gs:
            self.human_gs.eval()
                
        methods = ['hugs', 'hugs_human']
        metrics = ['lpips', 'psnr', 'ssim']
        metrics = dict.fromkeys(['_'.join(x) for x in itertools.product(methods, metrics)])
        metrics = {k: [] for k in metrics}
        
        # For FID calculation
        real_images_full = []
        fake_images_full = []
        real_images_human = []
        fake_images_human = []
        
        for idx, data in enumerate(tqdm(self.val_dataset, desc="Validation")):
            human_pack, scene_gs_out = None, None
            render_mode = self.cfg.mode
            
            if self.human_gs:
                human_pack = self.human_gs.forward(
                    global_orient=data['global_orient'],
                    body_pose=data['body_pose'],
                    betas=data['betas'],
                    transl=data['transl'],
                    smpl_scale=data['smpl_scale'][None],
                    dataset_idx=-1,
                    is_train=False,
                    ext_tfs=None,
                )

                body_out  = human_pack["body"] if isinstance(human_pack, dict) else human_pack
                cloth_out = human_pack.get("cloth", None) if isinstance(human_pack, dict) else None
            else:
                body_out, cloth_out = None, None

                
            if self.scene_gs:
                if iter is not None:
                    if iter >= self.cfg.scene.opt_start_iter:
                        scene_gs_out = self.scene_gs.forward()
                    else:
                        render_mode = 'human'
                else:
                    scene_gs_out = self.scene_gs.forward()
                    
            render_pkg = render_human_scene(
                data=data,
                human_gs_out=body_out,
                scene_gs_out=scene_gs_out,
                bg_color=bg_color,
                render_mode=render_mode,
                cloth_gs_out=cloth_out,   # <<< NEW
            )
            gt_image = data['rgb']
            
            image = render_pkg["render"]
            if self.cfg.dataset.name == 'zju':
                image = image * data['mask']
                gt_image = gt_image * data['mask']
            
            metrics['hugs_psnr'].append(psnr(image, gt_image).mean().double())
            metrics['hugs_ssim'].append(ssim(image, gt_image).mean().double())
            metrics['hugs_lpips'].append(self.lpips(image.clip(max=1), gt_image).mean().double())
            
            # Collect images for FID calculation
            real_images_full.append(gt_image.cpu())
            fake_images_full.append(image.cpu())
            
            log_img = torchvision.utils.make_grid([gt_image, image], nrow=2, pad_value=1)
            imf = f'{self.cfg.logdir}/val/full_{iter_s}_{idx:03d}.png'
            os.makedirs(os.path.dirname(imf), exist_ok=True)
            torchvision.utils.save_image(log_img, imf)
            
            log_img = []
            if self.cfg.mode in ['human', 'human_scene']:
                bbox = data['bbox'].to(int)
                cropped_gt_image = gt_image[:, bbox[0]:bbox[2], bbox[1]:bbox[3]]
                cropped_image = image[:, bbox[0]:bbox[2], bbox[1]:bbox[3]]
                log_img += [cropped_gt_image, cropped_image]
                
                metrics['hugs_human_psnr'].append(psnr(cropped_image, cropped_gt_image).mean().double())
                metrics['hugs_human_ssim'].append(ssim(cropped_image, cropped_gt_image).mean().double())
                metrics['hugs_human_lpips'].append(self.lpips(cropped_image.clip(max=1), cropped_gt_image).mean().double())
                
                
                cropped_gt_resized = torch.nn.functional.interpolate(
                    cropped_gt_image.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False
                ).squeeze(0)
                cropped_resized = torch.nn.functional.interpolate(
                    cropped_image.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False
                ).squeeze(0)
                
                real_images_human.append(cropped_gt_resized.cpu())
                fake_images_human.append(cropped_resized.cpu())
            
            if len(log_img) > 0:
                log_img = torchvision.utils.make_grid(log_img, nrow=len(log_img), pad_value=1)
                torchvision.utils.save_image(log_img, f'{self.cfg.logdir}/val/human_{iter_s}_{idx:03d}.png')
        
        
        self.eval_metrics[iter_s] = {}
        
        for k, v in metrics.items():
            if v == []:
                continue
            
            logger.info(f"{iter_s} - {k.upper()}: {torch.stack(v).mean().item():.4f}")
            self.eval_metrics[iter_s][k] = torch.stack(v).mean().item()
        
        # Calculate FID scores
        logger.info("Computing FID scores...")
        try:
            if len(real_images_full) > 0:
                fid_full = self.fid_calculator.calculate_fid(real_images_full, fake_images_full)
                logger.info(f"{iter_s} - HUGS_FID (full scene): {fid_full:.4f}")
                self.eval_metrics[iter_s]['hugs_fid'] = fid_full
            
            if len(real_images_human) > 0:
                fid_human = self.fid_calculator.calculate_fid(real_images_human, fake_images_human)
                logger.info(f"{iter_s} - HUGS_HUMAN_FID (cropped): {fid_human:.4f}")
                self.eval_metrics[iter_s]['hugs_human_fid'] = fid_human
        except Exception as e:
            logger.warning(f"FID calculation failed: {e}")
        
        torch.save(metrics, f'{self.cfg.logdir}/val/eval_{iter_s}.pth')
    
    @torch.no_grad()
    def animate(self, iter=None, keep_images=False):
        if self.anim_dataset is None:
            logger.info("No animation dataset found")
            return 0
        
        iter_s = 'final' if iter is None else f'{iter:06d}'
        if self.human_gs:
            self.human_gs.eval()
        
        os.makedirs(f'{self.cfg.logdir}/anim/', exist_ok=True)
        
        for idx, data in enumerate(tqdm(self.anim_dataset, desc="Animation")):
            human_gs_out, scene_gs_out = None, None
            
            if self.human_gs:
                ext_tfs = (data['manual_trans'], data['manual_rotmat'], data['manual_scale'])
                if self.human_gs:
                    human_pack = self.human_gs.forward(
                        smpl_scale=data['smpl_scale'][None],
                        is_train=False,
                        ext_tfs=None,
                    )

                    body_out  = human_pack["body"] if isinstance(human_pack, dict) else human_pack
                    cloth_out = human_pack.get("cloth", None) if isinstance(human_pack, dict) else None
                else:
                    body_out, cloth_out = None, None

            
            if self.scene_gs:
                scene_gs_out = self.scene_gs.forward()
                    
            render_pkg = render_human_scene(
                data=data,
                human_gs_out=body_out,
                scene_gs_out=scene_gs_out,
                bg_color=self.bg_color,
                render_mode=self.cfg.mode,
                cloth_gs_out=cloth_out,   # <<< NEW
            )

            
            image = render_pkg["render"]
            
            torchvision.utils.save_image(image, f'{self.cfg.logdir}/anim/{idx:05d}.png')
            
        video_fname = f'{self.cfg.logdir}/anim_{self.cfg.dataset.name}_{self.cfg.dataset.seq}_{iter_s}.mp4'
        create_video(f'{self.cfg.logdir}/anim/', video_fname, fps=20)
        if not keep_images:
            # shutil.rmtree(f'{self.cfg.logdir}/anim/')
            os.makedirs(f'{self.cfg.logdir}/anim/', exist_ok=True)
    
    @torch.no_grad()
    def render_canonical(self, iter=None, nframes=100, is_train_progress=False, pose_type=None):
        iter_s = 'final' if iter is None else f'{iter:06d}'
        iter_s += f'_{pose_type}' if pose_type is not None else ''
        
        if self.human_gs:
            self.human_gs.eval()
        
        os.makedirs(f'{self.cfg.logdir}/canon/', exist_ok=True)
        
        camera_params = get_rotating_camera(
            dist=5.0, img_size=256 if is_train_progress else 512, 
            nframes=nframes, device='cuda',
            angle_limit=torch.pi if is_train_progress else 2*torch.pi,
        )
        
        betas = self.human_gs.betas.detach() if hasattr(self.human_gs, 'betas') else self.train_dataset.betas[0]
        
        static_smpl_params = get_smpl_static_params(
            betas=betas,
            pose_type=self.cfg.human.canon_pose_type if pose_type is None else pose_type,
        )
        
        if is_train_progress:
            progress_imgs = []
        
        pbar = range(nframes) if is_train_progress else tqdm(range(nframes), desc="Canonical:")
        
        for idx in pbar:
            human_pack, scene_gs_out = None, None
            
            cam_p = camera_params[idx]
            data = dict(static_smpl_params, **cam_p)

            if self.human_gs:
                human_pack = self.human_gs.forward(
                    global_orient=data['global_orient'],
                    body_pose=data['body_pose'],
                    betas=data['betas'],
                    transl=data['transl'],
                    smpl_scale=data['smpl_scale'],
                    dataset_idx=-1,
                    is_train=False,
                    ext_tfs=None,
                )
                if isinstance(human_pack, dict):
                    body_out  = human_pack.get("body", None)
                    cloth_out = human_pack.get("cloth", None)
                else:
                    body_out, cloth_out = human_pack, None    
            if is_train_progress:
                scale_mod = 0.5
                render_pkg = render_human_scene(
                    data=data, 
                    human_gs_out=body_out, 
                    scene_gs_out=scene_gs_out, 
                    bg_color=self.bg_color,
                    render_mode='human',
                    scaling_modifier=scale_mod,
                    cloth_gs_out=cloth_out, 
                )
                
                image = render_pkg["render"]
                
                progress_imgs.append(image)
                
                render_pkg = render_human_scene(
                    data=data, 
                    human_gs_out=body_out, 
                    scene_gs_out=scene_gs_out, 
                    bg_color=self.bg_color,
                    render_mode='human',
                    cloth_gs_out=cloth_out, 
                )
                
                image = render_pkg["render"]
                
                progress_imgs.append(image)
                
            else:
                render_pkg = render_human_scene(
                    data=data, 
                    human_gs_out=body_out, 
                    scene_gs_out=scene_gs_out, 
                    bg_color=self.bg_color,
                    render_mode='human',
                    cloth_gs_out=cloth_out, 
                )
                
                image = render_pkg["render"]
                
                torchvision.utils.save_image(image, f'{self.cfg.logdir}/canon/{idx:05d}.png')
        
        if is_train_progress:
            os.makedirs(f'{self.cfg.logdir}/train_progress/', exist_ok=True)
            log_img = torchvision.utils.make_grid(progress_imgs, nrow=4, pad_value=0)
            save_image(log_img, f'{self.cfg.logdir}/train_progress/{iter:06d}.png', 
                       text_labels=f"{iter:06d}, n_gs={self.human_gs.n_gs}")
            return
        
        video_fname = f'{self.cfg.logdir}/canon_{self.cfg.dataset.name}_{self.cfg.dataset.seq}_{iter_s}.mp4'
        create_video(f'{self.cfg.logdir}/canon/', video_fname, fps=10)
        # shutil.rmtree(f'{self.cfg.logdir}/canon/')
        os.makedirs(f'{self.cfg.logdir}/canon/', exist_ok=True)
        
    def render_poses(self, camera_params, smpl_params, pose_type='a_pose', bg_color='white'):
    
        if self.human_gs:
            self.human_gs.eval()
        
        betas = self.human_gs.betas.detach() if hasattr(self.human_gs, 'betas') else self.val_dataset.betas[0]
        
        nframes = len(camera_params)
        
        canon_forward_out = None
        if hasattr(self.human_gs, 'canon_forward'):
            canon_forward_out = self.human_gs.canon_forward()
        
        pbar = tqdm(range(nframes), desc="Canonical:")
        if bg_color == 'white':
            bg_color = torch.tensor([1, 1, 1], dtype=torch.float32, device="cuda")
        elif bg_color == 'black':
            bg_color = torch.tensor([0, 0, 0], dtype=torch.float32, device="cuda")
            
            
        imgs = []
        for idx in pbar:
            human_pack, scene_gs_out = None, None
            
            cam_p = camera_params[idx]
            data = dict(smpl_params, **cam_p)

            if self.human_gs:
                if canon_forward_out is not None:
                    human_pack = self.human_gs.forward_test(
                        canon_forward_out,
                        global_orient=data['global_orient'],
                        body_pose=data['body_pose'],
                        betas=data['betas'],
                        transl=data['transl'],
                        smpl_scale=data['smpl_scale'],
                        dataset_idx=-1,
                        is_train=False,
                        ext_tfs=None,
                    )
                else:
                    human_pack = self.human_gs.forward(
                        global_orient=data['global_orient'],
                        body_pose=data['body_pose'],
                        betas=data['betas'],
                        transl=data['transl'],
                        smpl_scale=data['smpl_scale'],
                        dataset_idx=-1,
                        is_train=False,
                        ext_tfs=None,
                    )
           
            body_out  = human_pack["body"]
            cloth_out = human_pack.get("cloth", None)

            render_pkg = render_human_scene(
                data=data, 
                human_gs_out=body_out, 
                cloth_gs_out=cloth_out, 
                scene_gs_out=scene_gs_out, 
                bg_color=self.bg_color,
                render_mode='human',
            )
            image = render_pkg["render"]
            imgs.append(image)
        return imgs