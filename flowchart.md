# HUGS Feature-NonRigid-FiLM-Optimized Branch Flowchart

## ğŸš€ Main Entry Point
```
main.py
â”œâ”€â”€ Parse CLI arguments (--cfg_file, --cfg_id)
â”œâ”€â”€ Load config file (OmegaConf.load)
â”œâ”€â”€ Expand configs (get_cfg_items) - handles list-valued hyperparameters
â”œâ”€â”€ For each expanded config:
    â””â”€â”€ main(cfg)
        â”œâ”€â”€ safe_state(seed)
        â”œâ”€â”€ get_logger(cfg) - setup logging directories
        â”œâ”€â”€ GaussianTrainer(cfg)
        â”œâ”€â”€ trainer.train() (if not eval)
        â”œâ”€â”€ trainer.save_ckpt()
        â”œâ”€â”€ trainer.validate()
        â”œâ”€â”€ trainer.animate() (if human mode)
        â””â”€â”€ trainer.render_canonical() (a_pose, da_pose)
```

## ğŸ—ï¸ GaussianTrainer Initialization
```
GaussianTrainer.__init__(cfg)
â”œâ”€â”€ Load Datasets:
â”‚   â”œâ”€â”€ train_dataset = NeumanDataset(seq, 'train', ...)
â”‚   â”œâ”€â”€ val_dataset = NeumanDataset(seq, 'val', ...)
â”‚   â””â”€â”€ anim_dataset = NeumanDataset(seq, 'anim', ...)
â”œâ”€â”€ Initialize LPIPS loss
â”œâ”€â”€ Model Creation:
â”‚   â”œâ”€â”€ HUGS_TRIMLP (if cfg.human.name == 'hugs_trimlp')
â”‚   â”‚   â”œâ”€â”€ TriPlane(n_features=32, res=256)
â”‚   â”‚   â”œâ”€â”€ AppearanceDecoder(n_features=96)
â”‚   â”‚   â”œâ”€â”€ GeometryDecoder(n_features=96)
â”‚   â”‚   â”œâ”€â”€ DeformationDecoder(n_features=96)
â”‚   â”‚   â”œâ”€â”€ FiLM Components (if num_frames > 0):
â”‚   â”‚   â”‚   â”œâ”€â”€ frame_emb = nn.Embedding(num_frames, 32)
â”‚   â”‚   â”‚   â””â”€â”€ film_layer = nn.Linear(32, 2*96)
â”‚   â”‚   â””â”€â”€ SMPL Layer
â”‚   â””â”€â”€ SceneGS (if scene mode)
â”œâ”€â”€ Setup Optimizers
â”œâ”€â”€ Load Checkpoints (if available)
â””â”€â”€ Initialize SMPL Parameters
```

## ğŸ¯ Training Loop
```
train()
â”œâ”€â”€ RandomIndexIterator for data sampling
â”œâ”€â”€ For t_iter in range(num_steps):
â”‚   â”œâ”€â”€ Update learning rates
â”‚   â”œâ”€â”€ Sample random data index
â”‚   â”œâ”€â”€ Get data = train_dataset[rnd_idx]
â”‚   â”œâ”€â”€ Forward Pass:
â”‚   â”‚   â”œâ”€â”€ human_gs_out = human_gs.forward(...)
â”‚   â”‚   â””â”€â”€ scene_gs_out = scene_gs.forward() (if scene mode)
â”‚   â”œâ”€â”€ Rendering:
â”‚   â”‚   â””â”€â”€ render_pkg = render_human_scene(...)
â”‚   â”œâ”€â”€ Loss Computation:
â”‚   â”‚   â””â”€â”€ loss, loss_dict = loss_fn(...)
â”‚   â”œâ”€â”€ Backward Pass:
â”‚   â”‚   â””â”€â”€ loss.backward()
â”‚   â”œâ”€â”€ Optimization:
â”‚   â”‚   â”œâ”€â”€ human_gs.optimizer.step()
â”‚   â”‚   â””â”€â”€ scene_gs.optimizer.step() (if scene mode)
â”‚   â”œâ”€â”€ Densification (if conditions met):
â”‚   â”‚   â”œâ”€â”€ human_densification(...)
â”‚   â”‚   â””â”€â”€ scene_densification(...)
â”‚   â”œâ”€â”€ Save Progress Images (every 1000 steps)
â”‚   â”œâ”€â”€ Save Checkpoints (every save_ckpt_interval)
â”‚   â”œâ”€â”€ Validation (every val_interval)
â”‚   â”œâ”€â”€ Animation (every anim_interval)
â”‚   â””â”€â”€ Canonical Rendering (every 1000 steps)
```

## ğŸ§  HUGS_TRIMLP Forward Pass
```
HUGS_TRIMLP.forward(...)
â”œâ”€â”€ TriPlane Feature Extraction:
â”‚   â”œâ”€â”€ tri_feats = self.triplane(self.get_xyz)
â”‚   â””â”€â”€ Output: [N, 96] features (3 planes Ã— 32 features)
â”œâ”€â”€ FiLM Modulation (if enabled):
â”‚   â”œâ”€â”€ frame_idx = dataset_idx
â”‚   â”œâ”€â”€ cond = self.frame_emb(frame_idx)
â”‚   â”œâ”€â”€ gamma_beta = self.film_layer(cond)
â”‚   â”œâ”€â”€ gamma = gamma_beta[:96], beta = gamma_beta[96:]
â”‚   â””â”€â”€ tri_feats = tri_feats * (1 + gamma) + beta
â”œâ”€â”€ Decoder Networks:
â”‚   â”œâ”€â”€ appearance_out = self.appearance_dec(tri_feats)
â”‚   â”œâ”€â”€ geometry_out = self.geometry_dec(tri_feats)
â”‚   â””â”€â”€ deformation_out = self.deformation_dec(tri_feats) (if use_deformer)
â”œâ”€â”€ Gaussian Parameters:
â”‚   â”œâ”€â”€ xyz_offsets = geometry_out['xyz']
â”‚   â”œâ”€â”€ gs_rot6d = geometry_out['rotations']
â”‚   â”œâ”€â”€ gs_scales = geometry_out['scales']
â”‚   â”œâ”€â”€ gs_xyz = self.get_xyz + xyz_offsets
â”‚   â”œâ”€â”€ gs_rotmat = rotation_6d_to_matrix(gs_rot6d)
â”‚   â”œâ”€â”€ gs_rotq = matrix_to_quaternion(gs_rotmat)
â”‚   â”œâ”€â”€ gs_opacity = appearance_out['opacity']
â”‚   â””â”€â”€ gs_shs = appearance_out['shs']
â”œâ”€â”€ Non-Rigid Deformation (if use_deformer):
â”‚   â”œâ”€â”€ lbs_weights = deformation_out['lbs_weights']
â”‚   â”œâ”€â”€ posedirs = deformation_out['posedirs']
â”‚   â”œâ”€â”€ SMPL forward pass
â”‚   â”œâ”€â”€ LBS transformation: vitruvian â†’ t-pose â†’ posed
â”‚   â””â”€â”€ deformed_xyz = LBS(deformed_xyz, lbs_weights, posedirs)
â””â”€â”€ Return: {xyz, rotq, scales, opacity, shs, active_sh_degree}
```

## ğŸ¨ TriPlane Module
```
TriPlane.forward(x)
â”œâ”€â”€ Normalize coordinates: x = (x - center) / scale + 0.5
â”œâ”€â”€ Grid Sampling:
â”‚   â”œâ”€â”€ feat_xy = F.grid_sample(plane_xy, coords[..., [0, 1]])
â”‚   â”œâ”€â”€ feat_xz = F.grid_sample(plane_xz, coords[..., [0, 2]])
â”‚   â””â”€â”€ feat_yz = F.grid_sample(plane_yz, coords[..., [1, 2]])
â”œâ”€â”€ Concatenate features: feat = torch.cat([feat_xy, feat_xz, feat_yz], dim=1)
â””â”€â”€ Return: [N, 96] features
```

## ğŸ­ Rendering Pipeline
```
render_human_scene(...)
â”œâ”€â”€ Combine Gaussian Parameters:
â”‚   â”œâ”€â”€ human_gs_out: {xyz, rotq, scales, opacity, shs}
â”‚   â””â”€â”€ scene_gs_out: {xyz, rotq, scales, opacity, shs}
â”œâ”€â”€ Concatenate for rendering:
â”‚   â”œâ”€â”€ means3D = torch.cat([human_xyz, scene_xyz])
â”‚   â”œâ”€â”€ opacity = torch.cat([human_opacity, scene_opacity])
â”‚   â”œâ”€â”€ scales = torch.cat([human_scales, scene_scales])
â”‚   â””â”€â”€ rotations = torch.cat([human_rotq, scene_rotq])
â”œâ”€â”€ Gaussian Rasterization:
â”‚   â”œâ”€â”€ rasterizer = GaussianRasterizer(settings)
â”‚   â””â”€â”€ rendered_image, radii = rasterizer(...)
â””â”€â”€ Return: {render, viewspace_points, visibility_filter, radii}
```

## ğŸ“Š Loss Computation
```
HumanSceneLoss.forward(...)
â”œâ”€â”€ Extract ground truth and predicted images
â”œâ”€â”€ Apply masks based on render_mode
â”œâ”€â”€ Compute Losses:
â”‚   â”œâ”€â”€ L1 Loss: l1_loss(pred_img, gt_image)
â”‚   â”œâ”€â”€ SSIM Loss: 1.0 - ssim(pred_img, gt_image)
â”‚   â”œâ”€â”€ LPIPS Loss: lpips(pred_img, gt_image)
â”‚   â”œâ”€â”€ LBS Loss: lbs_regularization (if use_deformer)
â”‚   â””â”€â”€ Human Separation Loss (if human_scene mode)
â”œâ”€â”€ Weighted combination: total_loss = Î£(weight_i Ã— loss_i)
â””â”€â”€ Return: {total_loss, loss_dict, extras_dict}
```

## ğŸ”„ Densification Process
```
human_densification(...)
â”œâ”€â”€ Update max_radii2D based on visibility
â”œâ”€â”€ Add densification stats
â”œâ”€â”€ Densify and Prune (if conditions met):
â”‚   â”œâ”€â”€ densify_and_split(grads, threshold)
â”‚   â”œâ”€â”€ densify_and_clone(grads, threshold)
â”‚   â””â”€â”€ prune_points(mask)
â””â”€â”€ Update optimizer with new parameters
```

## ğŸ“¸ Validation & Animation
```
validate()
â”œâ”€â”€ Set models to eval mode
â”œâ”€â”€ For each validation sample:
â”‚   â”œâ”€â”€ Forward pass with validation data
â”‚   â”œâ”€â”€ Render image
â”‚   â”œâ”€â”€ Compute metrics (PSNR, SSIM, LPIPS)
â”‚   â””â”€â”€ Save validation images
â””â”€â”€ Save metrics to file

animate()
â”œâ”€â”€ Load animation dataset
â”œâ”€â”€ For each animation frame:
â”‚   â”œâ”€â”€ Forward pass with animation data
â”‚   â”œâ”€â”€ Render image
â”‚   â””â”€â”€ Save animation frame
â””â”€â”€ Create video from frames

render_canonical()
â”œâ”€â”€ Generate rotating camera parameters
â”œâ”€â”€ For each camera angle:
â”‚   â”œâ”€â”€ Forward pass with canonical pose
â”‚   â”œâ”€â”€ Render image
â”‚   â””â”€â”€ Save canonical view
â””â”€â”€ Create video from canonical views
```

## ğŸ—‚ï¸ Data Flow
```
NeumanDataset
â”œâ”€â”€ Load SMPL parameters (betas, body_pose, global_orient, transl)
â”œâ”€â”€ Load camera parameters (intrinsics, extrinsics)
â”œâ”€â”€ Load RGB images and masks
â”œâ”€â”€ Cache data to CUDA
â””â”€â”€ __getitem__: return data sample

Data Sample Structure:
{
    'rgb': [3, H, W],
    'mask': [H, W],
    'betas': [10],
    'body_pose': [23*3],
    'global_orient': [3],
    'transl': [3],
    'smpl_scale': scalar,
    'camera_center': [3],
    'world_view_transform': [4, 4],
    'full_proj_transform': [4, 4],
    'fovx': scalar,
    'fovy': scalar,
    'image_height': int,
    'image_width': int
}
```

## ğŸ”§ Key Optimizations in Feature-NonRigid-FiLM-Optimized

### 1. **FiLM Modulation**
- **Purpose**: Per-frame feature modulation for better temporal consistency
- **Implementation**: 
  - Frame embedding: `nn.Embedding(num_frames, 32)`
  - FiLM layer: `nn.Linear(32, 2*96)` â†’ gamma and beta
  - Modulation: `features * (1 + gamma) + beta`

### 2. **Non-Rigid Deformation**
- **Purpose**: Handle complex human motion and deformation
- **Components**:
  - DeformationDecoder: Predicts LBS weights and pose-dependent deformations
  - LBS transformation: vitruvian pose â†’ t-pose â†’ posed
  - Posedirs: Pose-dependent vertex offsets

### 3. **TriPlane Feature Representation**
- **Purpose**: Efficient 3D feature encoding
- **Structure**: 3 orthogonal planes (XY, XZ, YZ) with 32 features each
- **Output**: 96-dimensional feature vector per point

### 4. **Multi-Decoder Architecture**
- **AppearanceDecoder**: Predicts opacity and spherical harmonics
- **GeometryDecoder**: Predicts position offsets, rotations, and scales
- **DeformationDecoder**: Predicts LBS weights and pose-dependent deformations

## ğŸ¯ Training Modes

### Human Mode (`mode='human'`)
- Train only human model
- Render only human Gaussians
- Background color handling

### Scene Mode (`mode='scene'`)
- Train only scene model
- Render only scene Gaussians
- Point cloud initialization

### Human-Scene Mode (`mode='human_scene'`)
- Train both human and scene models
- Render combined scene
- Separate human/scene loss computation

## ğŸ“ Output Structure
```
output/
â”œâ”€â”€ human/
â”‚   â””â”€â”€ neuman/
â”‚       â””â”€â”€ {sequence}/
â”‚           â””â”€â”€ hugs_trimlp/
â”‚               â””â”€â”€ {exp_name}/
â”‚                   â””â”€â”€ {timestamp}/
â”‚                       â”œâ”€â”€ train/          # Training progress images
â”‚                       â”œâ”€â”€ val/            # Validation images
â”‚                       â”œâ”€â”€ anim/           # Animation frames
â”‚                       â”œâ”€â”€ canon/          # Canonical views
â”‚                       â”œâ”€â”€ meshes/         # Gaussian splat files
â”‚                       â”œâ”€â”€ ckpt/           # Model checkpoints
â”‚                       â””â”€â”€ logs/           # Training logs
```

## ğŸ”„ Configuration System
```
Config Expansion (get_cfg_items)
â”œâ”€â”€ Load base config from YAML
â”œâ”€â”€ Expand list-valued parameters
â”œâ”€â”€ Create Cartesian product of all combinations
â”œâ”€â”€ Generate experiment names
â””â”€â”€ Return list of configs to run

Example:
- dataset.seq: ['citron', 'parkinglot']
- human.sh_degree: [0, 1]
â†’ Creates 4 experiments: citron-sh0, citron-sh1, parkinglot-sh0, parkinglot-sh1
```

This flowchart represents the complete architecture and data flow of your `feature-nonrigid-film-optimized` branch, showing how the FiLM modulation, non-rigid deformation, and TriPlane features work together to create a sophisticated human rendering system. 